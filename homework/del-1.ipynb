{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f242c892",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip3 install python-docx pdfplumber pandas openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11fd298",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # å…³é”®è¯è¯å…¸\n",
    "\n",
    "# INVEST_HUMAN = {\n",
    "#     'æ•™è‚²': ['ä¹‰åŠ¡æ•™è‚²', 'é«˜ä¸­æ•™è‚²', 'èŒä¸šæ•™è‚²', 'èŒæ•™', 'é«˜ç­‰æ•™è‚²', 'æ•™å¸ˆåŸ¹è®­', 'å­¦ç”Ÿèµ„åŠ©',\n",
    "#             'è¯¾åæœåŠ¡', 'äº§æ•™èåˆ', 'æ•™è‚²ä¿¡æ¯åŒ–', 'ç‰¹å²—æ•™å¸ˆ', 'è¥å…»åˆé¤', 'å­¦ä½ä¾›ç»™'],\n",
    "#     'å«ç”Ÿå¥åº·': ['å…¬å…±å«ç”Ÿ', 'ç–«æƒ…é˜²æ§', 'åŸºå±‚åŒ»ç–—', 'åŒ»ä¿', 'å¦‡å¹¼ä¿å¥', 'ç²¾ç¥å«ç”Ÿ',\n",
    "#                 'å®¶åº­åŒ»ç”Ÿ', 'å¥åº·ä¿ƒè¿›', 'ä¸­åŒ»è¯', 'ç–¾æ§', 'åŒ»ç–—èƒ½åŠ›æå‡'],\n",
    "#     'ç¤¾ä¼šä¿éšœ': ['å…»è€é‡‘', 'ä½ä¿', 'ç‰¹å›°ä¾›å…»', 'æ®‹ç–¾äºº', 'æ‰˜è‚²', 'å„¿ç«¥ç¦åˆ©', 'å°±ä¸šåŸ¹è®­',\n",
    "#                 'åˆ›ä¸šæ‰¶æŒ', 'å…¬ç›Šæ€§å²—ä½', 'ç¨³å²—è¡¥è´´', 'æŠ€èƒ½åŸ¹è®­'],\n",
    "#     'äººæ‰å‘å±•': ['äººæ‰å¼•è¿›', 'å®‰å®¶è´¹', 'ç§Ÿæˆ¿è¡¥è´´', 'ç§‘ç ”å¯åŠ¨', 'é«˜å±‚æ¬¡äººæ‰', 'é’å¹´å‘å±•',\n",
    "#                 'æŠ€èƒ½äººæ‰', 'å·¥åŒ '],\n",
    "#     'æ–‡åŒ–æ²»ç†': ['å…¬å…±æ–‡åŒ–', 'ç¤¾åŒºæ²»ç†', 'æ³•å¾‹æ´åŠ©', 'å¿—æ„¿æœåŠ¡', 'æ•°å­—ç´ å…»']\n",
    "# }\n",
    "\n",
    "# INVEST_PHYSICAL = {\n",
    "#     'åŸºç¡€è®¾æ–½': ['äº¤é€š', 'é“è·¯', 'æ¡¥æ¢', 'è½¨é“äº¤é€š', 'åœ°é“', 'æœºåœº', 'æ¸¯å£', 'æ°´åˆ©', 'ç”µç½‘',\n",
    "#               '5G', 'æ•°æ®ä¸­å¿ƒ', 'æ–°åŸºå»º', 'æ™ºæ…§åŸå¸‚', 'åœ°ä¸‹ç®¡å»Š', 'åœè½¦åœº', 'å……ç”µæ¡©'],\n",
    "#     'äº§ä¸šå›­åŒº': ['äº§ä¸šå›­', 'å¼€å‘åŒº', 'é«˜æ–°åŒº', 'å·¥ä¸šå›­', 'æ ‡å‡†å‚æˆ¿', 'ç‰©æµåŸºåœ°', 'å†·é“¾',\n",
    "#               'å­µåŒ–å™¨', 'å¹³å°å…¬å¸', 'åœŸåœ°æ”¶å‚¨'],\n",
    "#     'å…¬å…±å»ºç­‘': ['æ–°å»º.*å­¦æ ¡', 'æ–°å»º.*åŒ»é™¢', 'æ–‡ä½“åœºé¦†', 'å…¬å›­', 'åŸå¸‚æ›´æ–°', 'æ£šæ”¹',\n",
    "#               'å®‰ç½®æˆ¿', 'ä¿éšœæˆ¿å»ºè®¾']  # æ³¨æ„ï¼šç”¨æ­£åˆ™åŒ¹é…\n",
    "# }\n",
    "\n",
    "# # æ‰å¹³åŒ–å…³é”®è¯åˆ—è¡¨ï¼ˆç”¨äºå¿«é€ŸåŒ¹é…ï¼‰\n",
    "# HUMAN_KEYWORDS = [kw for sublist in INVEST_HUMAN.values() for kw in sublist]\n",
    "# PHYSICAL_KEYWORDS = [kw for sublist in INVEST_PHYSICAL.values() for kw in sublist]\n",
    "\n",
    "# # ç¼–è¯‘æ­£åˆ™è¡¨è¾¾å¼ï¼ˆæ”¯æŒæ¨¡ç³ŠåŒ¹é…ï¼Œå¦‚â€œæ–°å»ºXXå­¦æ ¡â€ï¼‰\n",
    "# import re\n",
    "# PHYSICAL_PATTERNS = [re.compile(kw) if '.*' in kw else re.compile(re.escape(kw)) for kw in PHYSICAL_KEYWORDS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4566486d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # analyze_reports_all_formats.py\n",
    "\n",
    "# import os\n",
    "# import re\n",
    "# import pandas as pd\n",
    "# import warnings\n",
    "# import textract  # ç”¨äº .doc / .docx / .pdfï¼ˆå¤‡ç”¨ï¼‰\n",
    "# import pdfplumber  # ä»ä¼˜å…ˆç”¨ pdfplumber è§£æ PDFï¼ˆæ›´ç²¾å‡†ï¼‰\n",
    "\n",
    "# # å¿½ç•¥ pdfplumber è­¦å‘Š\n",
    "# warnings.filterwarnings(\"ignore\", message=\"Cannot render horizontal string\")\n",
    "\n",
    "# # ================== é…ç½® ==================\n",
    "# BASE_DIR = \"./reports\"\n",
    "# OUTPUT_CSV = \"./output/investment_classification.csv\"\n",
    "\n",
    "# def get_region_label(name):\n",
    "#     name = name.strip()\n",
    "#     if \"ä¸œéƒ¨\" in name: return \"ä¸œéƒ¨\"\n",
    "#     if \"ä¸­éƒ¨\" in name: return \"ä¸­éƒ¨\"\n",
    "#     if \"è¥¿éƒ¨\" in name: return \"è¥¿éƒ¨\"\n",
    "#     return \"æœªçŸ¥åŒºåŸŸ\"\n",
    "\n",
    "# # å…³é”®è¯ï¼ˆåŒå‰ï¼‰\n",
    "# HUMAN_KEYWORDS = [\n",
    "#     'ä¹‰åŠ¡æ•™è‚²', 'èŒä¸šæ•™è‚²', 'æ•™å¸ˆåŸ¹è®­', 'å­¦ç”Ÿèµ„åŠ©', 'è¯¾åæœåŠ¡', 'äº§æ•™èåˆ',\n",
    "#     'å…¬å…±å«ç”Ÿ', 'åŒ»ä¿', 'å¦‡å¹¼ä¿å¥', 'å®¶åº­åŒ»ç”Ÿ', 'å¥åº·ä¿ƒè¿›',\n",
    "#     'å…»è€é‡‘', 'ä½ä¿', 'æ‰˜è‚²', 'å°±ä¸šåŸ¹è®­', 'åˆ›ä¸šæ‰¶æŒ', 'æŠ€èƒ½åŸ¹è®­',\n",
    "#     'äººæ‰å¼•è¿›', 'å®‰å®¶è´¹', 'ç§Ÿæˆ¿è¡¥è´´', 'é«˜å±‚æ¬¡äººæ‰', 'é’å¹´å‘å±•',\n",
    "#     'å…¬å…±æ–‡åŒ–', 'ç¤¾åŒºæ²»ç†'\n",
    "# ]\n",
    "\n",
    "# PHYSICAL_PATTERNS = []\n",
    "# for kw in [\n",
    "#     'äº¤é€š', 'é“è·¯', 'æ¡¥æ¢', 'è½¨é“äº¤é€š', 'åœ°é“', 'æœºåœº', 'æ¸¯å£', 'æ°´åˆ©', 'ç”µç½‘',\n",
    "#     '5G', 'æ•°æ®ä¸­å¿ƒ', 'æ–°åŸºå»º', 'æ™ºæ…§åŸå¸‚', 'åœ°ä¸‹ç®¡å»Š', 'åœè½¦åœº',\n",
    "#     'äº§ä¸šå›­', 'å¼€å‘åŒº', 'é«˜æ–°åŒº', 'æ ‡å‡†å‚æˆ¿', 'ç‰©æµåŸºåœ°',\n",
    "#     r'æ–°å»º.*å­¦æ ¡', r'æ–°å»º.*åŒ»é™¢', r'åŸå¸‚æ›´æ–°', r'æ£šæ”¹', r'ä¿éšœæˆ¿'\n",
    "# ]:\n",
    "#     if '.*' in kw:\n",
    "#         PHYSICAL_PATTERNS.append(re.compile(kw))\n",
    "#     else:\n",
    "#         PHYSICAL_PATTERNS.append(re.compile(re.escape(kw)))\n",
    "\n",
    "# YEAR_PATTERN = re.compile(r'(202[1-5])')\n",
    "\n",
    "# # ================== æ–‡æœ¬æå–å‡½æ•° ==================\n",
    "\n",
    "# def extract_text_textract(filepath):\n",
    "#     \"\"\"é€šç”¨æ–‡æœ¬æå–ï¼ˆæ”¯æŒ .doc, .docx, .pdfï¼‰\"\"\"\n",
    "#     try:\n",
    "#         # textract è‡ªåŠ¨æ ¹æ®æ‰©å±•åé€‰æ‹©è§£æå™¨\n",
    "#         text = textract.process(filepath, encoding='utf-8')\n",
    "#         if isinstance(text, bytes):\n",
    "#             text = text.decode('utf-8', errors='ignore')\n",
    "#         return text\n",
    "#     except Exception as e:\n",
    "#         print(f\"  âš ï¸ textract å¤±è´¥ {os.path.basename(filepath)}: {str(e)[:100]}\")\n",
    "#         return \"\"\n",
    "\n",
    "# def extract_text_pdf_precise(filepath):\n",
    "#     \"\"\"ä¼˜å…ˆç”¨ pdfplumber æå– PDFï¼ˆæ›´å¯é ï¼‰\"\"\"\n",
    "#     pages = []\n",
    "#     try:\n",
    "#         with pdfplumber.open(filepath) as pdf:\n",
    "#             for i, page in enumerate(pdf.pages):\n",
    "#                 try:\n",
    "#                     text = page.extract_text(x_tolerance=1, y_tolerance=1) or \"\"\n",
    "#                     if text.strip():\n",
    "#                         pages.append((i + 1, text))\n",
    "#                 except:\n",
    "#                     continue\n",
    "#     except Exception as e:\n",
    "#         pass  # fallback to textract\n",
    "#     return pages\n",
    "\n",
    "# def classify_sentence(sent):\n",
    "#     if len(sent) < 15:\n",
    "#         return 'unclear', None\n",
    "#     for kw in HUMAN_KEYWORDS:\n",
    "#         if kw in sent:\n",
    "#             return 'human', kw\n",
    "#     for pat in PHYSICAL_PATTERNS:\n",
    "#         if pat.search(sent):\n",
    "#             return 'physical', pat.pattern\n",
    "#     return 'unclear', None\n",
    "\n",
    "# # ================== ä¸»æµç¨‹ ==================\n",
    "\n",
    "# all_records = []\n",
    "\n",
    "# for region_folder in os.listdir(BASE_DIR):\n",
    "#     region_path = os.path.join(BASE_DIR, region_folder)\n",
    "#     if not os.path.isdir(region_path):\n",
    "#         continue\n",
    "#     region_label = get_region_label(region_folder)\n",
    "#     print(f\"ğŸ“ åŒºåŸŸ: {region_folder} â†’ {region_label}\")\n",
    "\n",
    "#     for city_folder in os.listdir(region_path):\n",
    "#         city_path = os.path.join(region_path, city_folder)\n",
    "#         if not os.path.isdir(city_path):\n",
    "#             continue\n",
    "#         city_name = city_folder.rstrip('å¸‚')\n",
    "#         print(f\"  ğŸ™ï¸ åŸå¸‚: {city_name}\")\n",
    "\n",
    "#         for filename in os.listdir(city_path):\n",
    "#             filepath = os.path.join(city_path, filename)\n",
    "#             _, ext = os.path.splitext(filename)\n",
    "#             ext = ext.lower()\n",
    "\n",
    "#             if ext not in ['.doc', '.docx', '.pdf']:\n",
    "#                 continue\n",
    "\n",
    "#             # æå–å¹´ä»½\n",
    "#             year_match = YEAR_PATTERN.search(filename)\n",
    "#             if not year_match:\n",
    "#                 continue\n",
    "#             year = int(year_match.group(1))\n",
    "#             file_type = \"æ”¿åºœå·¥ä½œæŠ¥å‘Š\" if \"å·¥ä½œ\" in filename else \"è´¢æ”¿é¢„å†³ç®—æŠ¥å‘Š\"\n",
    "\n",
    "#             sentences_with_pages = []\n",
    "\n",
    "#             if ext == '.pdf':\n",
    "#                 # ä¼˜å…ˆç”¨ pdfplumber\n",
    "#                 pages = extract_text_pdf_precise(filepath)\n",
    "#                 if pages:\n",
    "#                     for page_num, text in pages:\n",
    "#                         sents = [s.strip() for s in re.split(r'[ã€‚ï¼›ï¼\\n]', text) if len(s) > 15]\n",
    "#                         for s in sents:\n",
    "#                             sentences_with_pages.append((page_num, s))\n",
    "#                 else:\n",
    "#                     # fallback to textract\n",
    "#                     text = extract_text_textract(filepath)\n",
    "#                     sents = [s.strip() for s in re.split(r'[ã€‚ï¼›ï¼\\n]', text) if len(s) > 15]\n",
    "#                     sentences_with_pages = [(None, s) for s in sents]\n",
    "#             else:\n",
    "#                 # .doc / .docx\n",
    "#                 text = extract_text_textract(filepath)\n",
    "#                 sents = [s.strip() for s in re.split(r'[ã€‚ï¼›ï¼\\n]', text) if len(s) > 15]\n",
    "#                 sentences_with_pages = [(None, s) for s in sents]\n",
    "\n",
    "#             # åˆ†ç±»\n",
    "#             for page_num, sent in sentences_with_pages:\n",
    "#                 category, matched_kw = classify_sentence(sent)\n",
    "#                 if category != 'unclear':\n",
    "#                     all_records.append({\n",
    "#                         \"åŒºåŸŸ\": region_label,\n",
    "#                         \"åŸå¸‚\": city_name,\n",
    "#                         \"å¹´ä»½\": year,\n",
    "#                         \"æ–‡ä»¶ç±»å‹\": file_type,\n",
    "#                         \"æ”¯å‡ºç±»åˆ«\": category,\n",
    "#                         \"åŒ¹é…å…³é”®è¯\": matched_kw,\n",
    "#                         \"åŸæ–‡ç‰‡æ®µ\": sent[:250].replace('\\n', ' '),\n",
    "#                         \"é¡µç \": page_num,\n",
    "#                         \"æ–‡ä»¶å\": filename\n",
    "#                     })\n",
    "\n",
    "# # ä¿å­˜ç»“æœ\n",
    "# os.makedirs(os.path.dirname(OUTPUT_CSV), exist_ok=True)\n",
    "# df = pd.DataFrame(all_records)\n",
    "# df.to_csv(OUTPUT_CSV, index=False, encoding='utf_8_sig')\n",
    "# print(f\"\\nâœ… å…±æå– {len(df)} æ¡è®°å½•ï¼Œå·²ä¿å­˜è‡³: {OUTPUT_CSV}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcca3624",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # analyze_all_reports.py\n",
    "# import os\n",
    "# import re\n",
    "# import pandas as pd\n",
    "# import warnings\n",
    "# import textract\n",
    "# import pdfplumber\n",
    "# from docx import Document\n",
    "\n",
    "# warnings.filterwarnings(\"ignore\", message=\"Cannot render horizontal string\")\n",
    "\n",
    "# # ================== é…ç½® ==================\n",
    "# BASE_DIR = \"./reports\"\n",
    "# OUTPUT_CSV = \"./output/investment_analysis.csv\"\n",
    "\n",
    "# # åŒºåŸŸæ˜ å°„ï¼ˆæ¨¡ç³ŠåŒ¹é…ï¼‰\n",
    "# def get_region_label(name):\n",
    "#     name = name.strip()\n",
    "#     if \"ä¸œéƒ¨\" in name: return \"ä¸œéƒ¨\"\n",
    "#     if \"ä¸­éƒ¨\" in name: return \"ä¸­éƒ¨\"\n",
    "#     if \"è¥¿éƒ¨\" in name: return \"è¥¿éƒ¨\"\n",
    "#     return \"æœªçŸ¥åŒºåŸŸ\"\n",
    "\n",
    "# # === å…³é”®è¯å®šä¹‰ ===\n",
    "# HUMAN_KEYWORDS = [\n",
    "#     'ä¹‰åŠ¡æ•™è‚²', 'é«˜ä¸­', 'èŒä¸šæ•™è‚²', 'èŒæ•™', 'é«˜ç­‰æ•™è‚²', 'æ•™å¸ˆ', 'å­¦ç”Ÿèµ„åŠ©',\n",
    "#     'è¯¾åæœåŠ¡', 'äº§æ•™èåˆ', 'æ•™è‚²ä¿¡æ¯åŒ–', 'ç‰¹å²—æ•™å¸ˆ', 'è¥å…»åˆé¤',\n",
    "#     'å…¬å…±å«ç”Ÿ', 'ç–«æƒ…é˜²æ§', 'åŸºå±‚åŒ»ç–—', 'åŒ»ä¿', 'å¦‡å¹¼', 'ç²¾ç¥å«ç”Ÿ',\n",
    "#     'å®¶åº­åŒ»ç”Ÿ', 'å¥åº·ä¿ƒè¿›', 'ä¸­åŒ»è¯', 'ç–¾æ§',\n",
    "#     'å…»è€é‡‘', 'ä½ä¿', 'ç‰¹å›°', 'æ®‹ç–¾äºº', 'æ‰˜è‚²', 'å„¿ç«¥ç¦åˆ©', 'å°±ä¸š',\n",
    "#     'åˆ›ä¸šæ‰¶æŒ', 'å…¬ç›Šæ€§å²—ä½', 'ç¨³å²—è¡¥è´´', 'æŠ€èƒ½åŸ¹è®­',\n",
    "#     'äººæ‰å¼•è¿›', 'å®‰å®¶è´¹', 'ç§Ÿæˆ¿è¡¥è´´', 'ç§‘ç ”å¯åŠ¨', 'é«˜å±‚æ¬¡äººæ‰', 'é’å¹´',\n",
    "#     'æŠ€èƒ½äººæ‰', 'å·¥åŒ ',\n",
    "#     'å…¬å…±æ–‡åŒ–', 'ç¤¾åŒºæ²»ç†', 'æ³•å¾‹æ´åŠ©', 'å¿—æ„¿æœåŠ¡'\n",
    "# ]\n",
    "\n",
    "# PHYSICAL_PATTERNS = []\n",
    "# for kw in [\n",
    "#     'äº¤é€š', 'é“è·¯', 'æ¡¥æ¢', 'è½¨é“äº¤é€š', 'åœ°é“', 'æœºåœº', 'æ¸¯å£', 'æ°´åˆ©', 'ç”µç½‘',\n",
    "#     '5G', 'æ•°æ®ä¸­å¿ƒ', 'æ–°åŸºå»º', 'æ™ºæ…§åŸå¸‚', 'åœ°ä¸‹ç®¡å»Š', 'åœè½¦åœº', 'å……ç”µæ¡©',\n",
    "#     'äº§ä¸šå›­', 'å¼€å‘åŒº', 'é«˜æ–°åŒº', 'å·¥ä¸šå›­', 'æ ‡å‡†å‚æˆ¿', 'ç‰©æµ', 'å†·é“¾',\n",
    "#     'å­µåŒ–å™¨', 'å¹³å°å…¬å¸', 'åœŸåœ°æ”¶å‚¨',\n",
    "#     r'æ–°å»º.*å­¦æ ¡', r'æ–°å»º.*åŒ»é™¢', r'æ–‡ä½“åœºé¦†', r'å…¬å›­', r'åŸå¸‚æ›´æ–°', r'æ£šæ”¹',\n",
    "#     r'å®‰ç½®æˆ¿', r'ä¿éšœæˆ¿'\n",
    "# ]:\n",
    "#     if '.*' in kw:\n",
    "#         PHYSICAL_PATTERNS.append(re.compile(kw))\n",
    "#     else:\n",
    "#         PHYSICAL_PATTERNS.append(re.compile(re.escape(kw)))\n",
    "\n",
    "# YEAR_PATTERN = re.compile(r'(202[1-5])')\n",
    "\n",
    "# # === è¡¨æ ¼è¯†åˆ«ï¼šæ˜¯å¦ä¸ºç›®æ ‡æ”¯å‡ºè¡¨ ===\n",
    "# def is_target_table(table):\n",
    "#     if not table or len(table) < 3:\n",
    "#         return False\n",
    "#     # æ£€æŸ¥å‰ä¸¤è¡Œæ ‡é¢˜\n",
    "#     header_text = \" \".join(str(cell) for row in table[:2] for cell in row if cell)\n",
    "#     if not re.search(r'(æ”¯å‡º.*å†³ç®—|åŠŸèƒ½åˆ†ç±»|ä¸€èˆ¬å…¬å…±é¢„ç®—)', header_text, re.IGNORECASE):\n",
    "#         return False\n",
    "#     # æ£€æŸ¥æ˜¯å¦æœ‰å…¸å‹æ”¯å‡ºç§‘ç›®\n",
    "#     for row in table[2:]:\n",
    "#         if not row or not row[0]:\n",
    "#             continue\n",
    "#         first_col = str(row[0]).strip()\n",
    "#         if any(kw in first_col for kw in ['æ•™è‚²', 'ç¤¾ä¼šä¿éšœ', 'å«ç”Ÿå¥åº·', 'åŸä¹¡ç¤¾åŒº', 'å†œæ—æ°´', 'äº¤é€šè¿è¾“', 'ä½æˆ¿ä¿éšœ']) and ('æ”¯å‡º' in first_col or 'åˆè®¡' in first_col):\n",
    "#             return True\n",
    "#     return False\n",
    "\n",
    "# # === æå–PDFä¸­çš„ç›®æ ‡è¡¨æ ¼ ===\n",
    "# def extract_target_tables_from_pdf(pdf_path):\n",
    "#     tables_found = []\n",
    "#     try:\n",
    "#         with pdfplumber.open(pdf_path) as pdf:\n",
    "#             for page_num, page in enumerate(pdf.pages, 1):\n",
    "#                 tables = page.extract_tables()\n",
    "#                 for table in tables:\n",
    "#                     if is_target_table(table):\n",
    "#                         tables_found.append((page_num, table))\n",
    "#     except Exception as e:\n",
    "#         pass\n",
    "#     return tables_found\n",
    "\n",
    "# # === æå–DOCXä¸­çš„ç›®æ ‡è¡¨æ ¼ ===\n",
    "# def extract_target_tables_from_docx(docx_path):\n",
    "#     try:\n",
    "#         doc = Document(docx_path)\n",
    "#         tables_found = []\n",
    "#         for table in doc.tables:\n",
    "#             data = []\n",
    "#             for row in table.rows:\n",
    "#                 data.append([cell.text.strip() for cell in row.cells])\n",
    "#             if is_target_table(data):\n",
    "#                 tables_found.append((None, data))  # Wordæ— é¡µç \n",
    "#         return tables_found\n",
    "#     except:\n",
    "#         return []\n",
    "\n",
    "# # === å¤„ç†æ”¯å‡ºè¡¨æ ¼ï¼ˆé€šç”¨ï¼‰===\n",
    "# def process_expense_table(table_data, city, year, region, filename, page=None):\n",
    "#     records = []\n",
    "#     df = pd.DataFrame(table_data[1:], columns=table_data[0])\n",
    "#     # å°è¯•æ‰¾åˆ°â€œé¡¹ç›®â€åˆ—å’Œâ€œé‡‘é¢â€åˆ—\n",
    "#     project_col = None\n",
    "#     amount_col = None\n",
    "#     for col in df.columns:\n",
    "#         col_str = str(col).lower()\n",
    "#         if 'é¡¹ç›®' in col_str or 'ç§‘ç›®' in col_str or 'åŠŸèƒ½' in col_str:\n",
    "#             project_col = col\n",
    "#         if 'å†³ç®—' in col_str or 'æ”¯å‡º' in col_str or 'é‡‘é¢' in col_str or 'æ•°' in col_str:\n",
    "#             amount_col = col\n",
    "#     if project_col is None or amount_col is None:\n",
    "#         return records\n",
    "\n",
    "#     for _, row in df.iterrows():\n",
    "#         project = str(row[project_col]).strip()\n",
    "#         if not project or project in ['åˆè®¡', 'æ€»è®¡', 'å°è®¡']:\n",
    "#             continue\n",
    "#         raw_amount = str(row[amount_col])\n",
    "#         # æ¸…ç†æ•°å­—ï¼šç§»é™¤éæ•°å­—å­—ç¬¦ï¼ˆä¿ç•™ . å’Œ -ï¼‰\n",
    "#         clean_amount = re.sub(r'[^\\d.\\-]', '', raw_amount)\n",
    "#         try:\n",
    "#             amount = float(clean_amount) if clean_amount else 0.0\n",
    "#         except:\n",
    "#             continue\n",
    "\n",
    "#         # åˆ†ç±»\n",
    "#         category = None\n",
    "#         if any(kw in project for kw in ['æ•™è‚²']):\n",
    "#             category = 'human'\n",
    "#         elif any(kw in project for kw in ['å«ç”Ÿ', 'å¥åº·', 'åŒ»ç–—']):\n",
    "#             category = 'human'\n",
    "#         elif any(kw in project for kw in ['ç¤¾ä¿', 'å…»è€', 'å°±ä¸š', 'æ•‘åŠ©', 'æ®‹ç–¾äºº', 'æ‰˜è‚²']):\n",
    "#             category = 'human'\n",
    "#         elif any(kw in project for kw in ['äººæ‰', 'å¼•è¿›', 'è¡¥è´´']):\n",
    "#             category = 'human'\n",
    "#         elif any(kw in project for kw in ['æ–‡åŒ–', 'æ—…æ¸¸', 'å®£ä¼ ']):\n",
    "#             category = 'human'\n",
    "#         else:\n",
    "#             category = 'physical'\n",
    "\n",
    "#         records.append({\n",
    "#             \"åŒºåŸŸ\": region,\n",
    "#             \"åŸå¸‚\": city,\n",
    "#             \"å¹´ä»½\": year,\n",
    "#             \"æ–‡ä»¶ç±»å‹\": \"è´¢æ”¿å†³ç®—æŠ¥å‘Š\",\n",
    "#             \"æ”¯å‡ºç±»åˆ«\": category,\n",
    "#             \"åŒ¹é…å…³é”®è¯\": project,\n",
    "#             \"åŸæ–‡ç‰‡æ®µ\": f\"{project}: {raw_amount}\",\n",
    "#             \"é‡‘é¢ï¼ˆä¸‡å…ƒï¼‰\": amount,\n",
    "#             \"é¡µç \": page,\n",
    "#             \"æ–‡ä»¶å\": filename\n",
    "#         })\n",
    "#     return records\n",
    "\n",
    "# # === å…¨æ–‡å¥å­åˆ†ç±» ===\n",
    "# def classify_sentence(sent):\n",
    "#     if len(sent) < 15:\n",
    "#         return 'unclear', None\n",
    "#     for kw in HUMAN_KEYWORDS:\n",
    "#         if kw in sent:\n",
    "#             return 'human', kw\n",
    "#     for pat in PHYSICAL_PATTERNS:\n",
    "#         if pat.search(sent):\n",
    "#             return 'physical', pat.pattern\n",
    "#     return 'unclear', None\n",
    "\n",
    "# # === ä¸»æµç¨‹ ===\n",
    "# all_records = []\n",
    "\n",
    "# for region_folder in os.listdir(BASE_DIR):\n",
    "#     region_path = os.path.join(BASE_DIR, region_folder)\n",
    "#     if not os.path.isdir(region_path): continue\n",
    "#     region_label = get_region_label(region_folder)\n",
    "#     print(f\"ğŸ“ åŒºåŸŸ: {region_folder} â†’ {region_label}\")\n",
    "\n",
    "#     for city_folder in os.listdir(region_path):\n",
    "#         city_path = os.path.join(region_path, city_folder)\n",
    "#         if not os.path.isdir(city_path): continue\n",
    "#         city_name = city_folder.rstrip('å¸‚')\n",
    "#         print(f\"  ğŸ™ï¸ åŸå¸‚: {city_name}\")\n",
    "\n",
    "#         for filename in os.listdir(city_path):\n",
    "#             filepath = os.path.join(city_path, filename)\n",
    "#             _, ext = os.path.splitext(filename)\n",
    "#             ext = ext.lower()\n",
    "\n",
    "#             if ext not in ['.doc', '.docx', '.pdf']:\n",
    "#                 continue\n",
    "\n",
    "#             year_match = YEAR_PATTERN.search(filename)\n",
    "#             if not year_match: continue\n",
    "#             year = int(year_match.group(1))\n",
    "\n",
    "#             if \"å·¥ä½œ\" in filename:\n",
    "#                 file_type = \"æ”¿åºœå·¥ä½œæŠ¥å‘Š\"\n",
    "#             elif \"å†³ç®—\" in filename or \"é¢„ç®—\" in filename:\n",
    "#                 file_type = \"è´¢æ”¿é¢„å†³ç®—æŠ¥å‘Š\"\n",
    "#             else:\n",
    "#                 continue\n",
    "\n",
    "#             # === å¤„ç†è´¢æ”¿é¢„å†³ç®—æŠ¥å‘Šï¼ˆä¼˜å…ˆè¡¨æ ¼ï¼‰===\n",
    "#             if file_type == \"è´¢æ”¿é¢„å†³ç®—æŠ¥å‘Š\":\n",
    "#                 tables = []\n",
    "#                 if ext == '.pdf':\n",
    "#                     tables = extract_target_tables_from_pdf(filepath)\n",
    "#                 elif ext == '.docx':\n",
    "#                     tables = extract_target_tables_from_docx(filepath)\n",
    "\n",
    "#                 if tables:\n",
    "#                     for page_num, table in tables:\n",
    "#                         records = process_expense_table(\n",
    "#                             table, city_name, year, region_label, filename, page_num\n",
    "#                         )\n",
    "#                         all_records.extend(records)\n",
    "#                     continue  # è¡¨æ ¼å·²å¤„ç†ï¼Œè·³è¿‡å…¨æ–‡\n",
    "\n",
    "#             # === å¤„ç†å…¨æ–‡ï¼ˆå·¥ä½œæŠ¥å‘Š æˆ– æ— è¡¨æ ¼çš„é¢„å†³ç®—ï¼‰===\n",
    "#             text = \"\"\n",
    "#             try:\n",
    "#                 if ext == '.pdf':\n",
    "#                     with pdfplumber.open(filepath) as pdf:\n",
    "#                         for page in pdf.pages:\n",
    "#                             txt = page.extract_text(x_tolerance=1, y_tolerance=1) or \"\"\n",
    "#                             text += txt + \"\\n\"\n",
    "#                 else:  # .doc / .docx\n",
    "#                     raw = textract.process(filepath, encoding='utf-8')\n",
    "#                     text = raw.decode('utf-8', errors='ignore') if isinstance(raw, bytes) else str(raw)\n",
    "#             except Exception as e:\n",
    "#                 print(f\"  âš ï¸ æ–‡æœ¬æå–å¤±è´¥ {filename}: {e}\")\n",
    "#                 continue\n",
    "\n",
    "#             sentences = [s.strip() for s in re.split(r'[ã€‚ï¼›ï¼\\n]', text) if len(s) > 15]\n",
    "#             for sent in sentences:\n",
    "#                 cat, kw = classify_sentence(sent)\n",
    "#                 if cat != 'unclear':\n",
    "#                     all_records.append({\n",
    "#                         \"åŒºåŸŸ\": region_label,\n",
    "#                         \"åŸå¸‚\": city_name,\n",
    "#                         \"å¹´ä»½\": year,\n",
    "#                         \"æ–‡ä»¶ç±»å‹\": file_type,\n",
    "#                         \"æ”¯å‡ºç±»åˆ«\": cat,\n",
    "#                         \"åŒ¹é…å…³é”®è¯\": kw,\n",
    "#                         \"åŸæ–‡ç‰‡æ®µ\": sent[:250].replace('\\n', ' '),\n",
    "#                         \"é‡‘é¢ï¼ˆä¸‡å…ƒï¼‰\": None,\n",
    "#                         \"é¡µç \": None,\n",
    "#                         \"æ–‡ä»¶å\": filename\n",
    "#                     })\n",
    "\n",
    "# # === ä¿å­˜ç»“æœ ===\n",
    "# os.makedirs(os.path.dirname(OUTPUT_CSV), exist_ok=True)\n",
    "# df = pd.DataFrame(all_records)\n",
    "# df.to_csv(OUTPUT_CSV, index=False, encoding='utf_8_sig')\n",
    "# print(f\"\\nâœ… å…±æå– {len(df)} æ¡è®°å½•ï¼Œç»“æœå·²ä¿å­˜è‡³: {OUTPUT_CSV}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76bc5b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Apple Silicon (M1/M2/M3)\n",
    "os.environ[\"PATH\"] = \"/opt/homebrew/bin:\" + os.environ.get(\"PATH\", \"\")\n",
    "\n",
    "# å¦‚æœæ˜¯ Intel Macï¼Œç”¨è¿™ä¸€è¡Œä»£æ›¿ï¼š\n",
    "# os.environ[\"PATH\"] = \"/usr/local/bin:\" + os.environ.get(\"PATH\", \"\")\n",
    "\n",
    "# éªŒè¯\n",
    "!which antiword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559c3510",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import textract\n",
    "import pdfplumber\n",
    "from docx import Document\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", message=\"Cannot render horizontal string\")\n",
    "\n",
    "# ================== é…ç½® ==================\n",
    "BASE_DIR = \"./reports\"\n",
    "OUTPUT_CSV = \"./output/investment_analysis.csv\"\n",
    "\n",
    "# åŒºåŸŸæ˜ å°„ï¼ˆæ¨¡ç³ŠåŒ¹é…ï¼‰\n",
    "def get_region_label(name):\n",
    "    name = name.strip()\n",
    "    if \"ä¸œéƒ¨\" in name: return \"ä¸œéƒ¨\"\n",
    "    if \"ä¸­éƒ¨\" in name: return \"ä¸­éƒ¨\"\n",
    "    if \"è¥¿éƒ¨\" in name: return \"è¥¿éƒ¨\"\n",
    "    return \"æœªçŸ¥åŒºåŸŸ\"\n",
    "\n",
    "# === å…³é”®è¯å®šä¹‰ ===\n",
    "HUMAN_KEYWORDS = [\n",
    "    'ä¹‰åŠ¡æ•™è‚²', 'é«˜ä¸­', 'èŒä¸šæ•™è‚²', 'èŒæ•™', 'é«˜ç­‰æ•™è‚²', 'æ•™å¸ˆ', 'å­¦ç”Ÿèµ„åŠ©',\n",
    "    'è¯¾åæœåŠ¡', 'äº§æ•™èåˆ', 'æ•™è‚²ä¿¡æ¯åŒ–', 'ç‰¹å²—æ•™å¸ˆ', 'è¥å…»åˆé¤',\n",
    "    'å…¬å…±å«ç”Ÿ', 'ç–«æƒ…é˜²æ§', 'åŸºå±‚åŒ»ç–—', 'åŒ»ä¿', 'å¦‡å¹¼', 'ç²¾ç¥å«ç”Ÿ',\n",
    "    'å®¶åº­åŒ»ç”Ÿ', 'å¥åº·ä¿ƒè¿›', 'ä¸­åŒ»è¯', 'ç–¾æ§',\n",
    "    'å…»è€é‡‘', 'ä½ä¿', 'ç‰¹å›°', 'æ®‹ç–¾äºº', 'æ‰˜è‚²', 'å„¿ç«¥ç¦åˆ©', 'å°±ä¸š',\n",
    "    'åˆ›ä¸šæ‰¶æŒ', 'å…¬ç›Šæ€§å²—ä½', 'ç¨³å²—è¡¥è´´', 'æŠ€èƒ½åŸ¹è®­',\n",
    "    'äººæ‰å¼•è¿›', 'å®‰å®¶è´¹', 'ç§Ÿæˆ¿è¡¥è´´', 'ç§‘ç ”å¯åŠ¨', 'é«˜å±‚æ¬¡äººæ‰', 'é’å¹´',\n",
    "    'æŠ€èƒ½äººæ‰', 'å·¥åŒ ',\n",
    "    'å…¬å…±æ–‡åŒ–', 'ç¤¾åŒºæ²»ç†', 'æ³•å¾‹æ´åŠ©', 'å¿—æ„¿æœåŠ¡'\n",
    "]\n",
    "\n",
    "PHYSICAL_PATTERNS = []\n",
    "for kw in [\n",
    "    'äº¤é€š', 'é“è·¯', 'æ¡¥æ¢', 'è½¨é“äº¤é€š', 'åœ°é“', 'æœºåœº', 'æ¸¯å£', 'æ°´åˆ©', 'ç”µç½‘',\n",
    "    '5G', 'æ•°æ®ä¸­å¿ƒ', 'æ–°åŸºå»º', 'æ™ºæ…§åŸå¸‚', 'åœ°ä¸‹ç®¡å»Š', 'åœè½¦åœº', 'å……ç”µæ¡©',\n",
    "    'äº§ä¸šå›­', 'å¼€å‘åŒº', 'é«˜æ–°åŒº', 'å·¥ä¸šå›­', 'æ ‡å‡†å‚æˆ¿', 'ç‰©æµ', 'å†·é“¾',\n",
    "    'å­µåŒ–å™¨', 'å¹³å°å…¬å¸', 'åœŸåœ°æ”¶å‚¨',\n",
    "    r'æ–°å»º.*å­¦æ ¡', r'æ–°å»º.*åŒ»é™¢', r'æ–‡ä½“åœºé¦†', r'å…¬å›­', r'åŸå¸‚æ›´æ–°', r'æ£šæ”¹',\n",
    "    r'å®‰ç½®æˆ¿', r'ä¿éšœæˆ¿'\n",
    "]:\n",
    "    if '.*' in kw:\n",
    "        PHYSICAL_PATTERNS.append(re.compile(kw))\n",
    "    else:\n",
    "        PHYSICAL_PATTERNS.append(re.compile(re.escape(kw)))\n",
    "\n",
    "YEAR_PATTERN = re.compile(r'(202[1-5])')\n",
    "\n",
    "# === è¡¨æ ¼è¯†åˆ«ï¼šæ˜¯å¦ä¸ºç›®æ ‡æ”¯å‡ºè¡¨ ===\n",
    "def is_target_table(table):\n",
    "    if not table or len(table) < 3:\n",
    "        return False\n",
    "    # æ£€æŸ¥å‰ä¸¤è¡Œæ ‡é¢˜\n",
    "    header_text = \" \".join(str(cell) for row in table[:2] for cell in row if cell)\n",
    "    if not re.search(r'(æ”¯å‡º.*å†³ç®—|åŠŸèƒ½åˆ†ç±»|ä¸€èˆ¬å…¬å…±é¢„ç®—)', header_text, re.IGNORECASE):\n",
    "        return False\n",
    "    # æ£€æŸ¥æ˜¯å¦æœ‰å…¸å‹æ”¯å‡ºç§‘ç›®\n",
    "    for row in table[2:]:\n",
    "        if not row or not row[0]:\n",
    "            continue\n",
    "        first_col = str(row[0]).strip()\n",
    "        if any(kw in first_col for kw in ['æ•™è‚²', 'ç¤¾ä¼šä¿éšœ', 'å«ç”Ÿå¥åº·', 'åŸä¹¡ç¤¾åŒº', 'å†œæ—æ°´', 'äº¤é€šè¿è¾“', 'ä½æˆ¿ä¿éšœ']) and ('æ”¯å‡º' in first_col or 'åˆè®¡' in first_col):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# === æå–PDFä¸­çš„ç›®æ ‡è¡¨æ ¼ ===\n",
    "def extract_target_tables_from_pdf(pdf_path):\n",
    "    tables_found = []\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            for page_num, page in enumerate(pdf.pages, 1):\n",
    "                tables = page.extract_tables()\n",
    "                for table in tables:\n",
    "                    if is_target_table(table):\n",
    "                        tables_found.append((page_num, table))\n",
    "    except Exception as e:\n",
    "        pass\n",
    "    return tables_found\n",
    "\n",
    "# === æå–DOCXä¸­çš„ç›®æ ‡è¡¨æ ¼ ===\n",
    "def extract_target_tables_from_docx(docx_path):\n",
    "    try:\n",
    "        doc = Document(docx_path)\n",
    "        tables_found = []\n",
    "        for table in doc.tables:\n",
    "            data = []\n",
    "            for row in table.rows:\n",
    "                data.append([cell.text.strip() for cell in row.cells])\n",
    "            if is_target_table(data):\n",
    "                tables_found.append((None, data))  # Wordæ— é¡µç \n",
    "        return tables_found\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "# === å¤„ç†æ”¯å‡ºè¡¨æ ¼ï¼ˆé€šç”¨ï¼‰===\n",
    "def process_expense_table(table_data, city, year, region, filename, page=None):\n",
    "    records = []\n",
    "    df = pd.DataFrame(table_data[1:], columns=table_data[0])\n",
    "    # å°è¯•æ‰¾åˆ°â€œé¡¹ç›®â€åˆ—å’Œâ€œé‡‘é¢â€åˆ—\n",
    "    project_col = None\n",
    "    amount_col = None\n",
    "    for col in df.columns:\n",
    "        col_str = str(col).lower()\n",
    "        if 'é¡¹ç›®' in col_str or 'ç§‘ç›®' in col_str or 'åŠŸèƒ½' in col_str:\n",
    "            project_col = col\n",
    "        if 'å†³ç®—' in col_str or 'æ”¯å‡º' in col_str or 'é‡‘é¢' in col_str or 'æ•°' in col_str:\n",
    "            amount_col = col\n",
    "    if project_col is None or amount_col is None:\n",
    "        return records\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        project = str(row[project_col]).strip()\n",
    "        if not project or project in ['åˆè®¡', 'æ€»è®¡', 'å°è®¡']:\n",
    "            continue\n",
    "        raw_amount = str(row[amount_col])\n",
    "        # æ¸…ç†æ•°å­—ï¼šç§»é™¤éæ•°å­—å­—ç¬¦ï¼ˆä¿ç•™ . å’Œ -ï¼‰\n",
    "        clean_amount = re.sub(r'[^\\d.\\-]', '', raw_amount)\n",
    "        try:\n",
    "            amount = float(clean_amount) if clean_amount else 0.0\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        # åˆ†ç±»\n",
    "        category = None\n",
    "        if any(kw in project for kw in ['æ•™è‚²']):\n",
    "            category = 'human'\n",
    "        elif any(kw in project for kw in ['å«ç”Ÿ', 'å¥åº·', 'åŒ»ç–—']):\n",
    "            category = 'human'\n",
    "        elif any(kw in project for kw in ['ç¤¾ä¿', 'å…»è€', 'å°±ä¸š', 'æ•‘åŠ©', 'æ®‹ç–¾äºº', 'æ‰˜è‚²']):\n",
    "            category = 'human'\n",
    "        elif any(kw in project for kw in ['äººæ‰', 'å¼•è¿›', 'è¡¥è´´']):\n",
    "            category = 'human'\n",
    "        elif any(kw in project for kw in ['æ–‡åŒ–', 'æ—…æ¸¸', 'å®£ä¼ ']):\n",
    "            category = 'human'\n",
    "        else:\n",
    "            category = 'physical'\n",
    "\n",
    "        records.append({\n",
    "            \"åŒºåŸŸ\": region,\n",
    "            \"åŸå¸‚\": city,\n",
    "            \"å¹´ä»½\": year,\n",
    "            \"æ–‡ä»¶ç±»å‹\": \"è´¢æ”¿å†³ç®—æŠ¥å‘Š\",\n",
    "            \"æ”¯å‡ºç±»åˆ«\": category,\n",
    "            \"åŒ¹é…å…³é”®è¯\": project,\n",
    "            \"åŸæ–‡ç‰‡æ®µ\": f\"{project}: {raw_amount}\",\n",
    "            \"é‡‘é¢ï¼ˆä¸‡å…ƒï¼‰\": amount,\n",
    "            \"é¡µç \": page,\n",
    "            \"æ–‡ä»¶å\": filename\n",
    "        })\n",
    "    return records\n",
    "\n",
    "# === å…¨æ–‡å¥å­åˆ†ç±» ===\n",
    "def classify_sentence(sent):\n",
    "    if len(sent) < 15:\n",
    "        return 'unclear', None\n",
    "    for kw in HUMAN_KEYWORDS:\n",
    "        if kw in sent:\n",
    "            return 'human', kw\n",
    "    for pat in PHYSICAL_PATTERNS:\n",
    "        if pat.search(sent):\n",
    "            return 'physical', pat.pattern\n",
    "    return 'unclear', None\n",
    "\n",
    "# === ä¸»æµç¨‹ ===# === ä¸»æµç¨‹ï¼ˆå¸¦å®æ—¶å†™å…¥ï¼‰===\n",
    "all_records = []  # å…¨å±€ç´¯è®¡\n",
    "\n",
    "# è·å–æ‰€æœ‰ (region, city) å¯¹ï¼Œä¾¿äºæ’åºå’Œè¿½è¸ª\n",
    "city_list = []\n",
    "for region_folder in os.listdir(BASE_DIR):\n",
    "    region_path = os.path.join(BASE_DIR, region_folder)\n",
    "    if not os.path.isdir(region_path): continue\n",
    "    for city_folder in os.listdir(region_path):\n",
    "        city_path = os.path.join(region_path, city_folder)\n",
    "        if os.path.isdir(city_path):\n",
    "            city_list.append((region_folder, city_folder))\n",
    "\n",
    "total_cities = len(city_list)\n",
    "print(f\"ğŸ“Š å…±å‘ç° {total_cities} ä¸ªåŸå¸‚ï¼Œå¼€å§‹å¤„ç†...\\n\")\n",
    "\n",
    "for idx, (region_folder, city_folder) in enumerate(city_list, 1):\n",
    "    region_path = os.path.join(BASE_DIR, region_folder)\n",
    "    city_path = os.path.join(region_path, city_folder)\n",
    "\n",
    "    region_label = get_region_label(region_folder)\n",
    "    city_name = city_folder.rstrip('å¸‚')\n",
    "\n",
    "    print(f\"[{idx}/{total_cities}] ğŸ™ï¸ æ­£åœ¨å¤„ç†: {city_name} ({region_label})\")\n",
    "\n",
    "    city_records = []\n",
    "\n",
    "    for filename in os.listdir(city_path):\n",
    "        # è·³è¿‡éšè—æ–‡ä»¶ï¼ˆä»¥ . å¼€å¤´ï¼‰å’Œä¸´æ—¶æ–‡ä»¶\n",
    "        if filename.startswith('.'):\n",
    "            continue\n",
    "        filepath = os.path.join(city_path, filename)\n",
    "        _, ext = os.path.splitext(filename)\n",
    "        ext = ext.lower()\n",
    "\n",
    "        if ext not in ['.doc', '.docx', '.pdf']:\n",
    "            continue\n",
    "\n",
    "        year_match = YEAR_PATTERN.search(filename)\n",
    "        if not year_match: continue\n",
    "        year = int(year_match.group(1))\n",
    "\n",
    "        if \"å·¥ä½œ\" in filename:\n",
    "            file_type = \"æ”¿åºœå·¥ä½œæŠ¥å‘Š\"\n",
    "        elif \"å†³ç®—\" in filename or \"é¢„ç®—\" in filename:\n",
    "            file_type = \"è´¢æ”¿é¢„å†³ç®—æŠ¥å‘Š\"\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        # === å¤„ç†è´¢æ”¿é¢„å†³ç®—æŠ¥å‘Šï¼ˆä¼˜å…ˆè¡¨æ ¼ï¼‰===\n",
    "        if file_type == \"è´¢æ”¿é¢„å†³ç®—æŠ¥å‘Š\":\n",
    "            tables = []\n",
    "            try:\n",
    "                if ext == '.pdf':\n",
    "                    tables = extract_target_tables_from_pdf(filepath)\n",
    "                elif ext == '.docx':\n",
    "                    # é¢å¤–æ£€æŸ¥ï¼šç¡®ä¿æ˜¯æœ‰æ•ˆ DOCXï¼ˆéä¸´æ—¶æ–‡ä»¶ï¼‰\n",
    "                    if os.path.getsize(filepath) < 1024:  # å°äº1KB è§†ä¸ºæ— æ•ˆ\n",
    "                        print(f\"  âš ï¸ è·³è¿‡ç©ºæ–‡ä»¶ {filename}\")\n",
    "                        continue\n",
    "                    tables = extract_target_tables_from_docx(filepath)\n",
    "            except Exception as e:\n",
    "                print(f\"  âš ï¸ è¡¨æ ¼æå–å¤±è´¥ {filename}: {e}\")\n",
    "                tables = []\n",
    "            if tables:\n",
    "                for page_num, table in tables:\n",
    "                    records = process_expense_table(\n",
    "                        table, city_name, year, region_label, filename, page_num\n",
    "                    )\n",
    "                    city_records.extend(records)\n",
    "                continue\n",
    "\n",
    "        # === å¤„ç†å…¨æ–‡ ===\n",
    "        text = \"\"\n",
    "        try:\n",
    "            if ext == '.pdf':\n",
    "                with pdfplumber.open(filepath) as pdf:\n",
    "                    for page in pdf.pages:\n",
    "                        txt = page.extract_text(x_tolerance=1, y_tolerance=1) or \"\"\n",
    "                        text += txt + \"\\n\"\n",
    "            else:\n",
    "                raw = textract.process(filepath, encoding='utf-8')\n",
    "                text = raw.decode('utf-8', errors='ignore') if isinstance(raw, bytes) else str(raw)\n",
    "        except Exception as e:\n",
    "             # ä¸å†æ‰“å°å®Œæ•´é”™è¯¯ï¼Œåªæç¤ºæ–‡ä»¶å\n",
    "            print(f\"  âš ï¸ è·³è¿‡æ— æ³•è¯»å–çš„æ–‡ä»¶: {filename}\")            \n",
    "            continue\n",
    "\n",
    "        sentences = [s.strip() for s in re.split(r'[ã€‚ï¼›ï¼\\n]', text) if len(s) > 15]\n",
    "        for sent in sentences:\n",
    "            cat, kw = classify_sentence(sent)\n",
    "            if cat != 'unclear' and \"æ”¶å…¥\" not in sent:\n",
    "                city_records.append({\n",
    "                    \"åŒºåŸŸ\": region_label,\n",
    "                    \"åŸå¸‚\": city_name,\n",
    "                    \"å¹´ä»½\": year,\n",
    "                    \"æ–‡ä»¶ç±»å‹\": file_type,\n",
    "                    \"æ”¯å‡ºç±»åˆ«\": cat,\n",
    "                    \"åŒ¹é…å…³é”®è¯\": kw,\n",
    "                    \"åŸæ–‡ç‰‡æ®µ\": sent[:250].replace('\\n', ' '),\n",
    "                    \"é‡‘é¢ï¼ˆä¸‡å…ƒï¼‰\": None,\n",
    "                    \"é¡µç \": None,\n",
    "                    \"æ–‡ä»¶å\": filename\n",
    "                })\n",
    "\n",
    "    # å°†å½“å‰åŸå¸‚ç»“æœåŠ å…¥å…¨å±€\n",
    "    all_records.extend(city_records)\n",
    "    print(f\"  âœ… å®Œæˆ {city_name}ï¼Œæ–°å¢ {len(city_records)} æ¡è®°å½•ï¼Œç´¯è®¡ {len(all_records)} æ¡\")\n",
    "\n",
    "    # === å®æ—¶å†™å…¥ CSVï¼ˆè¦†ç›–æ¨¡å¼ï¼‰===\n",
    "    os.makedirs(os.path.dirname(OUTPUT_CSV), exist_ok=True)\n",
    "    df = pd.DataFrame(all_records)\n",
    "    df.to_csv(OUTPUT_CSV, index=False, encoding='utf_8_sig')\n",
    "    print(f\"  ğŸ’¾ å·²ä¿å­˜è‡³: {OUTPUT_CSV}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
