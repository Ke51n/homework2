# import os
# import re
# import pandas as pd
# from tqdm import tqdm
 
# ================== é…ç½®åŒº ==================
# BASE_DIR = r"D:\AAanancoda\å¤§æ•°æ®æœŸæœ«ä½œä¸š\æ•°æ®"
 
# åˆ†ç±»è§„åˆ™ï¼štype + å¯é€‰é˜ˆå€¼ï¼ˆç”¨äºè¾¹ç•Œç§‘ç›®ï¼‰
# CATEGORY_RULES = {
# # æŠ•èµ„äºäººï¼ˆHuman Capitalï¼‰
# "æ•™è‚²": {"type": "human", "threshold": None},
# "ç§‘æŠ€": {"type": "human", "threshold": 1.0},      # â‰¤1äº¿è§†ä¸ºäººæ‰/ç ”å‘
# "ç§‘å­¦æŠ€æœ¯": {"type": "human", "threshold": 1.0},
# "ç§‘ç ”": {"type": "human", "threshold": 1.0},
# "ç¤¾ä¿": {"type": "human", "threshold": None},
# "ç¤¾ä¼šä¿éšœ": {"type": "human", "threshold": None},
# "å°±ä¸š": {"type": "human", "threshold": None},
# "å«ç”Ÿ": {"type": "human", "threshold": None},
# "åŒ»ç–—": {"type": "human", "threshold": None},
# "å¥åº·": {"type": "human", "threshold": None},
# "ç§Ÿèµè¡¥è´´": {"type": "human", "threshold": None},

#     # æŠ•èµ„äºç‰©ï¼ˆPhysical Capitalï¼‰
# "äº¤é€š": {"type": "physical", "threshold": None},
# "è¿è¾“": {"type": "physical", "threshold": None},
# "åŸä¹¡": {"type": "physical", "threshold": None},
# "ç¤¾åŒº": {"type": "physical", "threshold": None},
# "å¸‚æ”¿": {"type": "physical", "threshold": None},
# "å†œä¸š": {"type": "physical", "threshold": 5.0},   # >5äº¿è§†ä¸ºåŸºå»º
# "æ—ä¸š": {"type": "physical", "threshold": 5.0},
# "æ°´åˆ©": {"type": "physical", "threshold": 5.0},
# "å†œæ—": {"type": "physical", "threshold": 5.0},
# "èµ„æºå‹˜æ¢": {"type": "physical", "threshold": None},
# "å·¥ä¸š": {"type": "physical", "threshold": None},
# "ä¿¡æ¯": {"type": "physical", "threshold": None},
# "åˆ¶é€ ä¸š": {"type": "physical", "threshold": None},
# "ä½æˆ¿å»ºè®¾": {"type": "physical", "threshold": None},
# "æ£šæ”¹": {"type": "physical", "threshold": None},
# "ä¿éšœæˆ¿": {"type": "physical", "threshold": None},  # é»˜è®¤è®¡å…¥â€œç‰©â€
# }
 
# åŒä¹‰è¯æ˜ å°„ï¼ˆæ”¯æŒåœ°æ–¹è¡¨è¿°å·®å¼‚ï¼‰
# SYNONYMS = {
# "æ•™è‚²": ["æ•™è‚²"],
# "ç§‘æŠ€": ["ç§‘æŠ€", "ç§‘å­¦æŠ€æœ¯", "ç§‘ç ”"],
# "ç¤¾ä¿": ["ç¤¾ä¿", "ç¤¾ä¼šä¿éšœå’Œå°±ä¸š", "ç¤¾ä¼šæ•‘åŠ©", "å…»è€", "ä½ä¿"],
# "å«ç”Ÿ": ["å«ç”Ÿ", "åŒ»ç–—", "å«ç”Ÿå¥åº·", "å…¬å…±å«ç”Ÿ", "åŒ»é™¢", "åŸºå±‚åŒ»ç–—"],
# "äº¤é€š": ["äº¤é€š", "äº¤é€šè¿è¾“", "å…¬è·¯", "é«˜é€Ÿ", "åœ°é“", "é“è·¯", "æœºåœº"],
# "åŸä¹¡": ["åŸä¹¡", "åŸä¹¡ç¤¾åŒº", "å¸‚æ”¿", "åŸå¸‚æ›´æ–°", "åŸºç¡€è®¾æ–½"],
# "å†œä¸š": ["å†œä¸š", "å†œæ—æ°´", "æ°´åˆ©", "æ—ä¸š", "é«˜æ ‡å‡†å†œç”°", "å†œæ‘å…¬è·¯"],
# "ä½æˆ¿å»ºè®¾": ["ä¿éšœæˆ¿", "æ£šæ”¹", "ä½æˆ¿ä¿éšœ", "è€æ—§å°åŒºæ”¹é€ ", "å®‰å±…å·¥ç¨‹"]
# }
 
# æ„å»ºæœ€ç»ˆå…³é”®è¯ â†’ è§„åˆ™æ˜ å°„
# FINAL_KEYWORD_MAP = {}
# for main_kw, syns in SYNONYMS.items():
# rule = CATEGORY_RULES.get(main_kw, {"type": "physical", "threshold": None})  # é»˜è®¤å½’â€œç‰©â€
# for syn in syns:
# FINAL_KEYWORD_MAP[syn] = rule
 
# ================== æ ¸å¿ƒå‡½æ•° ==================
# ï¿¼
# def determine_target_year_and_type(file_path):
# """ä»æ–‡ä»¶è·¯å¾„è§£æï¼šåŸå¸‚ã€ç›®æ ‡å¹´ä»½ã€æ–‡ä»¶ç±»å‹"""
# try:
# rel_path = os.path.relpath(file_path, BASE_DIR)
# parts = rel_path.split(os.sep)
# if len(parts) < 3:
# return None, None, None
# city, folder_name, filename = parts[0], parts[1], parts[2]     year_match = re.search(r'(\d{4})', filename)
#     if not year_match:
#         return None, None, None
#     year = int(year_match.group(1))
    
#     if "å†³ç®—" in folder_name:
#         return city, year, "å†³ç®—"
#     elif "é¢„ç®—" in folder_name:
#         return city, year, "é¢„ç®—"
#     else:
#         return city, None, "æŠ¥å‘Š"
# except:
#     return None, None, None
# ï¿¼
# def extract_expenditure_amount(line):
# """
# é«˜ç²¾åº¦æå–æ”¯å‡ºé‡‘é¢ï¼ˆå•ä½ï¼šäº¿å…ƒï¼‰
# è¿”å› floatï¼Œæ— æœ‰æ•ˆé‡‘é¢è¿”å› 0.0
# """
# line = line.strip()
# if not line or len(line) < 8:
# return 0.0

#     # æ’é™¤æ˜æ˜¾éæ”¯å‡ºé‡‘é¢è¡Œ
# if re.search(râ€™(åŒæ¯”|å¢é•¿|ä¸‹é™|å®Œæˆç‡|å æ¯”|æ€»è®¡|åˆè®¡|æ€»æ”¯å‡º|ä½™é¢|ç»“ä½™|\d+%)â€™, line):
# return 0.0

#     # 1. ä¼˜å…ˆåŒ¹é…â€œXXäº¿å…ƒâ€æˆ–â€œXXäº¿â€
# match = re.search(râ€™([\d,]+.?\d*)\säº¿(?:å…ƒ)?â€™, line)
# if match:
# try:
# num_str = match.group(1).replace(â€™,â€™, â€˜â€™)
# num = float(num_str)
# if 0.1 <= num <= 2000:  # åˆç†è´¢æ”¿æ”¯å‡ºèŒƒå›´
# return num
# except (ValueError, TypeError):
# pass

#     # 2. åŒ¹é…â€œXXä¸‡å…ƒâ€ä¸” â‰¥1äº¿ï¼ˆå³ â‰¥10000ä¸‡å…ƒï¼‰
# match = re.search(râ€™(\d{5,})\sä¸‡å…ƒâ€™, line)  # è‡³å°‘5ä½æ•°å­—
# if match:
# try:
# num_wan = int(match.group(1))
# num_yi = num_wan / 10000.0
# if 1.0 <= num_yi <= 2000:
# return num_yi
# except (ValueError, TypeError):
# pass

#     # 3. å…³é”®è¯ + è¡Œå°¾æ•°å­—ï¼ˆä¿å®ˆç­–ç•¥ï¼‰
# if re.search(râ€™(?:æ•™è‚²|ç§‘æŠ€|ç§‘å­¦|ç¤¾ä¿|å«ç”Ÿ|åŒ»ç–—|äº¤é€š|è¿è¾“|åŸä¹¡|ç¤¾åŒº|å¸‚æ”¿|å†œä¸š|æ—ä¸š|æ°´åˆ©|å†œæ—|å·¥ä¸š|ä¿¡æ¯|ä½æˆ¿|æ£šæ”¹|ä¿éšœæˆ¿)â€™, line):
# match = re.search(râ€™(\d+.?\d*)\s* $ â€˜, line)
# if match:
# try:
# num = float(match.group(1))
# if 1.0 <= num <= 1000:  # å‡è®¾å•ä½ä¸ºäº¿å…ƒ
# return num
# except (ValueError, TypeError):
# pass

#     return 0.0

# def classify_expenditure(text):
# """ä»æ–‡æœ¬ä¸­åˆ†ç±»æ±‡æ€»æ”¯å‡º"""
# human_total = 0.0
# physical_total = 0.0
# lines = text.split(â€™\nâ€™)

#     for line in lines:
# amount = extract_expenditure_amount(line)
# if amount == 0.0:
# continue

#         # åŒ¹é…æ”¯å‡ºç§‘ç›®
# matched = False
# for keyword, rule in FINAL_KEYWORD_MAP.items():
# if keyword in line:
# # åº”ç”¨é˜ˆå€¼è§„åˆ™
# threshold = rule["threshold"]
# if threshold is not None:
# if (keyword in ["å†œä¸š", "æ—ä¸š", "æ°´åˆ©", "å†œæ—"] and amount > threshold) or 
# (keyword in ["ç§‘æŠ€", "ç§‘å­¦æŠ€æœ¯", "ç§‘ç ”"] and amount <= threshold):
# pass  # ç¬¦åˆæ¡ä»¶
# else:
# continue  # ä¸ç¬¦åˆæ¡ä»¶ï¼Œè·³è¿‡             # ç´¯åŠ 
#             if rule["type"] == "human":
#                 human_total += amount
#             else:
#                 physical_total += amount
#             matched = True
#             break  # ä¸€è¡Œåªè®¡ä¸€æ¬¡
# ï¿¼
#     return round(human_total, 2), round(physical_total, 2)
 
# ================== ä¸»æµç¨‹ ==================
# results = []
# txt_files = []
 
# æ”¶é›†æ‰€æœ‰ .txt æ–‡ä»¶
# for root, _, files in os.walk(BASE_DIR):
# for f in files:
# if f.endswith(".txt"):
# txt_files.append(os.path.join(root, f))

# print(f"ğŸ” å…±å‘ç° {len(txt_files)} ä¸ª TXT æ–‡ä»¶ï¼Œå¼€å§‹å¤„ç†â€¦")

# for file_path in tqdm(txt_files, desc="å¤„ç†æ–‡ä»¶"):
# try:
# city, year, file_type = determine_target_year_and_type(file_path)
# if year is None or file_type == "æŠ¥å‘Š":
# continue  # è·³è¿‡æ”¿åºœå·¥ä½œæŠ¥å‘Š

#         with open(file_path, "r", encoding="utf-8") as f:
# content = f.read()

#         human, physical = classify_expenditure(content)
# if human > 0 or physical > 0:
# results.append({
# "åŸå¸‚": city,
# "å¹´ä»½": year,
# "æ•°æ®æ¥æº": file_type,
# "æŠ•èµ„äºäºº_äº¿å…ƒ": human,
# "æŠ•èµ„äºç‰©_äº¿å…ƒ": physical
# })
# except Exception as e:
# print(f"\nâŒ å¤„ç†å¤±è´¥: {file_path} | é”™è¯¯: {str(e)[:120]}")
 
# ================== åå¤„ç†ï¼šå»é‡ + æ’åºï¼ˆå…¼å®¹æ—§ç‰ˆ pandasï¼‰ ==================
# if not results:
# print("âš ï¸ æœªæå–åˆ°ä»»ä½•æœ‰æ•ˆæ•°æ®ï¼Œè¯·æ£€æŸ¥ TXT æ–‡ä»¶å†…å®¹ã€‚")
# else:
# df = pd.DataFrame(results) # æ·»åŠ ä¸´æ—¶æ’åºåˆ—ï¼šå†³ç®—=0ï¼Œé¢„ç®—=1
# df["æ’åºä¼˜å…ˆçº§"] = df["æ•°æ®æ¥æº"].map({"å†³ç®—": 0, "é¢„ç®—": 1})
# df = df.sort_values(["åŸå¸‚", "å¹´ä»½", "æ’åºä¼˜å…ˆçº§"])
# df = df.drop_duplicates(subset=["åŸå¸‚", "å¹´ä»½"], keep="first")
# df = df.drop(columns=["æ’åºä¼˜å…ˆçº§"])  # åˆ é™¤ä¸´æ—¶åˆ—

# df = df.sort_values(["åŸå¸‚", "å¹´ä»½"]).reset_index(drop=True)
# ï¿¼
#     # ä¿å­˜ç»“æœ
# output_path = os.path.join(BASE_DIR, "..", "investment_analysis.xlsx")
# df.to_excel(output_path, index=False, engine="openpyxl")

#     print(f"\nâœ… å¤„ç†å®Œæˆï¼å…±æå– {len(df)} æ¡æœ‰æ•ˆè®°å½•")
# print(f"ğŸ“Š ç»“æœå·²ä¿å­˜è‡³: {output_path}")
# print("\nå‰5è¡Œé¢„è§ˆ:")
# print(df.head().to_string(index=False)) 